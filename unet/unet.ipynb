{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net is a convolutional neural network that was developed initially for biomedical image segmentation. This week, we will be working on implementing the U-Net architecture using PyTorch and performing an image segmentation task.\n",
    "\n",
    "This network is a bit more complicated than the MLP and simple CNN you have seen so far, but it is a nice example of what PyTorch allows to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper\n",
    "import simulation\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be working with a small dataset of synthetic images. Let us generate 2 images with their segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images have shape (2, 1, 80, 80). Min value in image: 0. Max value in image: 255.\n",
      "Image masks have shape (2, 5, 80, 80). Min value in mask: 0.0. Max value in mask: 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(80, 80, count=2)\n",
    "\n",
    "print(f\"Input images have shape {input_images.shape}. Min value in image: {input_images.min()}. Max value in image: {input_images.max()}.\")\n",
    "print(f\"Image masks have shape {target_masks.shape}. Min value in mask: {target_masks.min()}. Max value in mask: {target_masks.max()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input images have shape $(2, 1, 80, 80)$:  $2$ for the number of images, $1$ for the color channels (they are black and white in this case), and $(80,80)$ for the image height and width.\n",
    "\n",
    "The images contain 5 different shapes. For each input image we have a segmentation mask, with which we encode what shape the pixels in the image segment.\n",
    "\n",
    "The corresponding image masks have shape $(2, 5, 80, 80)$: $2$ for the number of images, $5$ for the one-hot encoding of what shape the pixel is a part of (see image below), and $(80,80)$ for the image height and width.\n",
    "\n",
    "Let us plot the image and the corresponding segmentation mask to get a better idea of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change channel-order and make 3 channels just for plotting\n",
    "input_images_rgb = [x.astype(np.uint8).repeat(3,axis=0).transpose(1,2,0) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image (black and white), Right: Target mask (5ch)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be using PyTorch's Dataset class for preparing the data. Our `SimDataset` class will inherit from PyTorch's dataset class. We need to overwrite the functions `__len__(self)`, and `__getitem__(self, idx)`.\n",
    "\n",
    "*  `__len__(self)`: returns the size of the dataset.\n",
    "\n",
    "* `__getitem__(self, idx)`: returns the data (and its target) sample of index `idx`.\n",
    "\n",
    "\n",
    "(Hint: The tutorial at https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class is very helpful and we encourage you to have a look at it. You can also add transformations for data augmentation etc. very easily!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count):\n",
    "        # We generate our data \n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(80, 80, count=count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        \n",
    "        return [image, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use PyTorch's `DataLoader` class. This is an iterator which allows us to batch the data, shuffle it and load it with multiprocessing workers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and use PyTorch's DataLoader class.\n",
    "train_set = SimDataset(300)\n",
    "val_set = SimDataset(20)\n",
    "test_set = SimDataset(3)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=20, shuffle=True)\n",
    "val_dataloader= DataLoader(val_set, batch_size=20, shuffle=True)\n",
    "test_dataloader= DataLoader(test_set, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Dataset and Dataloader, it is time to design our deep network! Since we are doing image segmentation, we choose to use a U-Net architecture here, which was first introduced in the paper \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" ([link to paper](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) for the interested). The original architecture looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unet.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quote the architecture description from the paper directly: \n",
    "\n",
    "> It consists of a contracting path (left side) and an expansive path (right side). \n",
    "\n",
    "> The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of **two 3x3 convolutions** (unpadded convolutions), each followed by a **rectified linear unit (ReLU)** and a **2x2 max pooling operation with stride 2** for downsampling. At each downsampling step we double the number of feature channels. \n",
    "\n",
    "\n",
    "> Every step in the expansive path consists of an **upsampling** of the feature map followed by a **2x2 convolution (“up-convolution”)** that halves the number of feature channels, a **concatenation with the correspondingly cropped feature map** from the contracting path, and **two 3x3 convolutions**, each followed by a **ReLU**. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes. In total the network has 23 convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we make matters slightly simpler by using **padded convolutions with padding of size 1**, so that the image doesn't become smaller after the convolution operations. This way, **we do not have to crop** the feature map from the contracting path for the concatenation. We will also build a slightly smaller network, since our images are not so large in size to begin with. The network we want to implement is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unet-2.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets code this! We would like to first create a module class for the double convolution operation (convolution -> batch norm -> ReLU -> convolution -> batch norm -> ReLU), since it is repeated several times in the architecture. This class inherits from PyTorch's module class. We use the [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to define the layers in this module. \n",
    "\n",
    "Just to show you how this works, we wrote the class for a single convolution followed by a Sigmoid operation called `DummyConv`. We then use this `DummyConv` module three times in the `DummyNetwork`. Study how this works. \n",
    "\n",
    "**Note:** [batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) is a technique for normalizing each batch of data within the network, to help its training. If you haven't seen it in the lectures, you don't need to know its details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyConv(nn.Module):\n",
    "    \"\"\"A small module that implements (convolution => sigmoid).\n",
    "    It will be re-used as a build block to make a full network.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):            \n",
    "        super().__init__()\n",
    "        \n",
    "        # We build a sequence of layers with nn.Sequential().\n",
    "        self.single_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.single_conv(x)\n",
    "\n",
    "\n",
    "class DummyNetwork(nn.Module):\n",
    "    \"\"\"A network that uses the DummyConv modules above.\"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.initial_layer = DummyConv(n_channels, 64)\n",
    "        self.second_layer = DummyConv(64, 128)\n",
    "        self.third_layer = DummyConv(128, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.initial_layer(x)\n",
    "        x2 = self.second_layer(x1)\n",
    "        x3 = self.third_layer(x2)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to implement the `DoubleConv` module. Use the template above as help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"A module that will implement the following sequence of layers:\n",
    "    (convolution => batch normalization => ReLU => convolution => batch normalization => ReLU).\n",
    "    Batch normalization is a technique for normalization over mini-batches. \n",
    "    To use it in a CNN, you can use PyTorch's nn.BatchNorm2d layer:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us implement the `UNet` class, as we did in previous week's exercise. We've started the implementation as a hint and left the rest for you to fill.\n",
    "\n",
    "For the **up-and-conv 2x2** operation of the network, as shown on the illustrations, we will be using **Transposed Convolutions**. You can take a look at [`nn.ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d) for the PyTorch implementation. *Hint:* You should take an interest in the arguments `kernel_size` and `stride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Beginning of the \"contracting\" part of the U-Net\n",
    "        self.dc1 = DoubleConv(input_channels, 64)\n",
    "        self.mp1 = nn.MaxPool2d(2)\n",
    "        self.dc2 = DoubleConv(64, 128)\n",
    "        \n",
    "        self.mp2 = nn.MaxPool2d(2)\n",
    "        self.dc3 = DoubleConv(128, 256)\n",
    "        self.mp3 = nn.MaxPool2d(2)\n",
    "        self.dc4 = DoubleConv(256, 512)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dc5 = DoubleConv(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dc6 = DoubleConv(256, 128)\n",
    "        ###\n",
    "        \n",
    "        # End of the \"expansive\" part of the U-Net\n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dc7 = DoubleConv(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.dc1(x)\n",
    "        x2 = self.mp1(x1)\n",
    "        x2 = self.dc2(x2)\n",
    "        \n",
    "        x3 = self.mp2(x2)\n",
    "        x3 = self.dc3(x3)\n",
    "        x4 = self.mp3(x3)\n",
    "        x4 = self.dc4(x4)\n",
    "\n",
    "        x = self.up1(x4)\n",
    "        x = torch.cat([x3, x], dim=1)\n",
    "        x = self.dc5(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.dc6(x)\n",
    "        ### \n",
    "                 \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.dc7(x)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(input_channels=1, n_classes=5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a sum of binary cross entropy losses and dice loss for our loss, since image segmentation is actually a classification problem (you can think of it as classifying what object the pixels belong to, if they belong to any.) You're already familiar with cross entropy loss from the lectures. Dice loss is a measure of overlap between the prediction and the ground truth labels and is also used for image segmentation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def full_loss(pred, target):\n",
    "    bce_weight = 0.5\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is given below. Fill in the missing part of the training loop! Call the `full_loss` function above to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs=10):\n",
    "    best_loss = 1e10\n",
    "    best_model_wts = None\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"LR\", param_group['lr'])\n",
    "\n",
    "        ##### TRAINING:\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0\n",
    "        epoch_samples = 0\n",
    "        #load the images and masks\n",
    "        for bi, (inputs, labels) in enumerate(train_dataloader):\n",
    "            print(f\"\\rProcessing batch {bi}/\"\n",
    "                  f\"{len(train_dataloader) - 1}\", end='')\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = full_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # statistics\n",
    "            epoch_samples +=  inputs.size(0)\n",
    "            epoch_loss += loss.data.cpu().numpy() * inputs.size(0)\n",
    "        print(\"Training epoch loss: {}\".format(epoch_loss/epoch_samples))\n",
    "        scheduler.step()\n",
    "\n",
    "        ##### VALIDATION:\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "        epoch_loss = 0\n",
    "        epoch_samples = 0\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            with torch.no_grad():  # disable gradient calculation\n",
    "                outputs = model(inputs)\n",
    "                loss = full_loss(outputs, labels)\n",
    "\n",
    "                # statistics\n",
    "                epoch_loss += loss.data.cpu().numpy() * inputs.size(0)\n",
    "                epoch_samples +=  inputs.size(0)\n",
    "        print(\"Val epoch loss: {}\".format(epoch_loss/epoch_samples))\n",
    "\n",
    "        # save the model if the loss is the best\n",
    "        if epoch_loss/epoch_samples < best_loss:\n",
    "            print(\"saving best model\")\n",
    "            best_loss = epoch_loss/epoch_samples\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, it is time to train! \n",
    "\n",
    "First we will create the optimizer. This time we will use the Adam optimizer ([optim.Adam](https://pytorch.org/docs/stable/optim.html)). Set the learning rate to 1e-2.\n",
    "\n",
    "We've also created a learning rate scheduler for you, which gradually reduces the learning rate during training, this can be useful to converge to a better solution. The intuition behind it is that the closer we get to a minima, the smaller the step we want to get closer and closer.\n",
    "\n",
    "If you have a GPU with CUDA support, this should be pretty fast! However, **it might take a while if you only train on the CPU**. If this is the case, try it with a reduced number of epochs first. You should be seeing some segmented objects after 3 epochs (but keep in mind that at this point the classification will be likely wrong, i.e. visually the colors will not match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.01\n",
      "Processing batch 6/14"
     ]
    }
   ],
   "source": [
    "model = UNet(input_channels=1, n_classes=5)\n",
    "model = model.to(device)\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-2)  ### WRITE YOUR CODE HERE\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.9)\n",
    "train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we do on the test data! We plot the ground truth masks and the predicted masks side by side below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch\n",
    "inputs, labels = next(iter(test_dataloader))\n",
    "inputs = inputs.to(device).float()\n",
    "labels = labels.to(device).float()\n",
    "\n",
    "# Predict\n",
    "pred = model(inputs)\n",
    "pred = torch.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.repeat(3,axis=0).transpose(1,2,0).astype(np.uint8) for x in inputs.cpu().numpy()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "print(\"We plot the input, target, and prediction for the test set:\")\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements:**\n",
    "\n",
    "This notebook contains a mix of usuyama's https://github.com/usuyama/pytorch-unet and milesial's https://github.com/milesial/Pytorch-UNet code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
